[{"content":"Github使用日志 在部署博客的过程中由于对github缺乏使用经验，导致了很多问题。于是决定在现在完成后记录一下github的使用。\n[TOC]\n2020年GitHub的日志数达到了8.6亿条，活跃代码仓库达到了5,421万个，活跃开发者数达到了1,454万人，拥有超过3,100万开发人员和9,600多万个存储库。\nGithub和Git的具体概念 首先，github和git是两个不同的概念。GitHub 本身是一个基于web的服务平台，其通过提供git仓库的托管进行服务。而git则是开源的分布式版本控制系统，两者绝对不能混为一谈。\ngit并不是github独有的。包括Bitbucket、SourceForge、Gogs、Gitbucket、GitLab、Gitee、Azure DevOps、Gitea在内的多个平台使用的都是git。\ngit的具体概念 Git 是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。\nGit 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。\nGit 与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。\n对象存储：Git使用内容寻址文件系统来存储内容。每个文件和目录都以对象的形式存储，并通过SHA-1哈希值进行索引。\n分支管理：在Git中，分支是一个引用（轻量级的分支）或是一个分支对象（重量级的分支）。分支切换实际上是改变当前HEAD指针的位置。\n索引（Index）：Git的索引是一个准备区，用于暂存即将提交的文件变更。\n冲突解决：当两个分支有冲突时，Git会标记出冲突的文件，需要手动解决冲突后才能进行合并。\n标签（Tag）：用于标记特定的提交，通常用于版本发布。\n仓库（Repository）：Git用来保存项目文件和版本历史的数据库。每个项目都有一个Git仓库。\n提交（Commit）：项目文件的一个快照，包括文件的内容和元数据（如作者、日期、提交信息）。\n分支（Branch）：指向特定提交的可移动的指针，用于隔离开发流程的不同部分。\n合并（Merge）：将两个或多个不同的开发历史合并在一起。\n克隆（Clone）：创建一个仓库的副本，包括所有文件和提交历史。\n远程仓库（Remote Repository）：托管在服务器上的仓库，可以是GitHub、GitLab等。\ngithub的具体概念 GitHub是一个面向开源及私有软件项目的托管平台，因为只支持Git作为唯一的版本库格式进行托管，故名GitHub。GitHub拥有1亿以上的开发人员，400万以上组织机构和3.3亿以上资料库。\n代码托管：GitHub允许用户托管Git仓库，并提供了一个图形界面来浏览代码、提交历史、分支和标签。\n协作工具：GitHub提供了issues（问题跟踪系统）、pull requests（代码审查和合并请求）、wikis（项目文档）和项目看板等工具，以支持团队协作。\n社交功能：GitHub有关注（following）、星标（starring）、观察（watching）等社交功能，允许用户跟踪项目和开发者的活动。\n集成和自动化：GitHub提供了API和Webhooks，允许开发者集成外部服务和自动化工作流程。\n代码审查和合并：通过pull requests，GitHub支持代码审查和讨论，确保代码质量，并简化合并流程。 Github的具体使用 GitHub允许你创建一个远程库。需要通过git来将本地的库同步到github中。\nGithub允许你提交一个SSH密钥到账号。SSH允许你无需账号密码来同步文件。\nGithub包含一个Issues，用于追踪项目中的错误和功能请求。可以在仓库的页面上找到New issue，填写相关信息后提交。\nGithub允许你通过Pull Requests来请求将某个分支的变更合并到主分支，便于代码审查。在仓库页面，点击\u0026quot;Pull requests\u0026quot;，然后点击\u0026quot;New pull request\u0026quot;，选择要合并的分支，添加更改说明后提交。\nGithub包含一个Wikis，可以在仓库中托管项目文档。在仓库页面，点击\u0026quot;Wiki\u0026quot;标签，然后点击\u0026quot;Add or edit pages\u0026quot;，创建或编辑文档页面。\nGithub包含GitHub Actions可以实现自动化部署和持续集成（CI/CD）。例如在同步仓库时自动更新readme等操作。若要使用，则需在仓库的.github/workflows目录下创建一个YAML文件，定义工作流程和触发条件。\nGithub还包含Stars（点赞/关注）、Forks（克隆）、Watching（订阅）等内容。这是一种用户间的互动。\nGithub可以创建组织，方便同步文件。登录GitHub账户，点击右上角的\u0026quot;+\u0026ldquo;号，选择\u0026quot;New organization\u0026rdquo;，填写组织信息后创建。在组织的页面，点击\u0026quot;Teams\u0026quot;，然后点击\u0026quot;New team\u0026quot;，设置团队名称和成员。\nGit的具体使用 git的本地仓库包含三个部分：其一是工作目录，其保存着实际的文件。其二是暂存区，类似于缓存，保存临时改动。其三是HEAD区，指向最后一次提交的结果。\ngit init：初始化一个git仓库。\ngit clone path：克隆一个本地仓库。把path换成具体路径。\ngit clone [url]：克隆一个远程仓库。包含https克隆和SSH克隆。https的链接通常类似于这样：https://github.com/username/repository.git。SSH的链接通常类似于这样：git clone git@github.com:username/repository.git。\ngit add \u0026lt;filename\u0026gt;：添加文件到暂存区。如果filename为**.**（就是一个点）就是指当前目录下的所有文件。文件名不添加路径则是当前目录下的文件。当选择的是文件夹会递归的添加其下的所有文件。\ngit add -u：这个命令只添加已经跟踪的文件（即之前已经添加到Git仓库的文件），不包括新文件。 git add -A / git add --all：这些命令添加所有变化的文件和新文件。 git commit -m \u0026quot;代码提交信息\u0026quot;：将改动提交到HEAD。\ngit push origin master：将这些改动推送到远端仓库。其中，origin是远程仓库的默认名称，当你克隆一个远程仓库时，Git 自动将远程仓库的引用设置为 origin。这个名称是可替的。master是提交分支名，可以自行更改。\ngit remote -v：查看远程仓库的URL，origin 会显示在列表中。\ngit remote add new_origin \u0026lt;repository_url\u0026gt;：添加一个新的远程仓库，命名为new_origin。\nmaster是git的默认分支。\ngit checkout feature_x：切换到某个分支。feature_x是该分支的名称。\ngit checkout -b feature_x：创建并切换到某个分支，feature_x是该分支的名称。\ngit branch -d feature_x：删除某个分支，feature_x是该分支的名称。\ngit push origin \u0026lt;branch\u0026gt;：推送这个分支。没有推送的分支在远程上是不可见的。\ngit pull [remote] [branch]:从远程仓库拉取代码变更，并尝试将这些变更自动合并到当前本地分支。[remote]：这是远程仓库的名称，默认是 origin。branch：这是远程仓库中你想要拉取的分支名称。如果你不指定 [remote] 和 branch，Git 会默认拉取 origin 远程仓库中与当前本地分支关联的分支的变更。\ngit merge \u0026lt;branch\u0026gt;：合并一个分支到当前分支。branch是该分支的名称（\u0026lt;\u0026gt;是不要的）。\ngit diff \u0026lt;source_branch\u0026gt; \u0026lt;target_branch\u0026gt;：预览两个分支的差异。\ngit log：获得提交ID。\ngit tag 1.0.0 id：创建一个叫做 1.0.0 的标签。id指提交 ID 的前 10 位字符。\ngit checkout -- \u0026lt;filename\u0026gt;：使用 HEAD 中的最新内容替换掉你的工作目录中的文件。\ngit fetch [remote]：从远程仓库获取数据，并下载远程分支的更新和提交，但不会自动合并这些更改到你的本地分支。\ngit reset [--hard] [\u0026lt;commit\u0026gt;]：重置当前HEAD和索引（暂存区）。[--hard]：这是一个可选的选项，表示重置时连同工作目录一起重置，即放弃所有本地未提交的更改。[\u0026lt;commit\u0026gt;]：这是一个占位符，表示你想要重置到的特定的提交（commit）。可以是一个分支名、标签或者提交的哈希值。\ngitk：图形化git。\ngit config color.ui true/false开启/关闭彩色输出。\ngit config format.pretty oneline：显示历史记录时，每个提交的信息只显示一行。\ngit add -i：交互式添加文件到暂存区。\n具体语法实例： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 # 初始化一个Git仓库 git init # 克隆一个远程仓库到本地 git clone https://github.com/username/repository.git # 添加文件到暂存区 git add index.md # 添加所有变化的文件和新文件 git add -A # 添加已经跟踪的文件（不包括新文件） git add -u # 提交暂存区的更改到本地仓库 git commit -m \u0026#34;Add index.md with Github usage log\u0026#34; # 查看远程仓库的URL git remote -v # 添加一个新的远程仓库引用 git remote add origin https://github.com/username/repository.git # 推送本地仓库的更改到远程仓库 git push -u origin master # 从远程仓库拉取代码变更，并合并到当前本地分支 git pull origin master # 合并远程分支的更改到当前分支 git merge origin/master # 显示两个分支的差异 git diff master feature_x # 查看提交历史 git log # 创建一个标签 git tag 1.0.0 \u0026lt;commit_id\u0026gt; # 检出标签对应的提交 git checkout 1.0.0 # 检出HEAD中的最新内容替换工作目录中的文件 git checkout -- index.md # 从远程仓库获取数据，但不自动合并 git fetch origin # 重置当前HEAD和索引（暂存区）到指定的提交 git reset --hard \u0026lt;commit_id\u0026gt; # 重置当前HEAD和索引（暂存区）到远程分支的状态 git reset --hard origin/master # 打开图形化Git工具 gitk # 开启/关闭彩色输出 git config --global color.ui true # 显示历史记录时，每个提交的信息只显示一行 git config --global format.pretty oneline # 交互式添加文件到暂存区 git add -i 总结 Git 作为一个开源的分布式版本控制系统，以其高效和灵活性被广泛应用于各种项目中。而 GitHub，作为一个基于 Web 的服务平台，提供了 Git 仓库托管和丰富的协作工具，极大地方便了开发者之间的代码共享和项目管理。 本篇涉及了了 Git 的核心特性，和基础语法，大概阐释了涉及到的概念。可以作为我自身的查档用，也有一定的参考价值。 GitHub 的主要语法和操作实际上基于 Git，而 GitHub 则作为远程仓库的角色，使得代码的远程托管和管理变得更加便捷。希望本文能帮助您更好地理解和使用 Git 和 GitHub，提高您的工作效率，并在开源社区中发挥更大的作用。 相关资料 * git - 简明指南 * Git 教程|菜鸟编程 * Github-百度百科 * Git-百度百科 * Git官方文档 * Git维基百科 * Github维基百科\n","date":"2025-01-02T00:00:00Z","image":"https://SJTdreams.github.io/p/github%E4%BD%BF%E7%94%A8%E6%97%A5%E5%BF%97/title_hu7573704500553638163.png","permalink":"https://SJTdreams.github.io/p/github%E4%BD%BF%E7%94%A8%E6%97%A5%E5%BF%97/","title":"Github使用日志"},{"content":"机器学习 线性代数学习笔记 本篇是在学习过程中写的，可能会存在疏漏，欢迎补充！\n“人工智能是我们人类正在从事的最为深刻的研究方向之一，甚至要比火与电还更加深刻。”\n​\t——桑德尔·皮猜（Sundar Pichai）, 2020\n[TOC]\n本人观看地址：https://www.bilibili.com/video/BV1Pg4y1X7Pa/?spm_id_from=333.337.search-card.all.click\n1. 数学基础在AI学习中的作用 1.1 数学基础在AI研究中的必要性 AI训练、学习和研究的的过程中存在大量涉及数学的要点。良好的数学基础是必要的\n理论支撑：存在大量的AI模型理论都依靠大量的数学基础。深度学习的原理上涉及到了大量的线性代数运算，如点积、矩阵乘法等。而微积分也在各种损失函数、计算梯度上运用广泛。例如训练神经网络的核心算法：反向传播，就涉及到了大量的微积分知识。大名鼎鼎的框架TensorFlow，直译就是“张量流动”。\n模型优化：在AI模型的训练过程中，优化算法如梯度下降法需要用到微积分的知识。数学能够帮助我们分析和选择最佳的学习率，从而加速模型的收敛，提高模型性能。\n数据分析：AI领域中的数据预处理、特征提取等步骤都涉及到统计学和概率论。这些数学工具帮助我们从数据中提取有价值的信息，为模型训练提供支持。例如，可用通过统计学识别异常值，处理缺失值等。而将数据标准化和归一化也涉及到数学。\n算法创新：数学是推动AI算法创新的关键。许多新的AI算法，如卷积神经网络（CNN）和循环神经网络（RNN），都是基于数学理论的创新。CNN运用了数学上的互相关运算，而RNN的时间序列也是矩阵表示的，RNN的预测还涉及到了马尔科夫链(概率论)。\n1.2 可能涉及到的数学内容 以下是各种数学领域可能涉及到的深度学习内容:\n线性代数：在线性代数中，矩阵和向量运算是构建和理解神经网络的基础。矩阵的乘法、转置、逆等操作在神经网络的前向传播和反向传播中扮演着核心角色。线性代数极大程度的简化了模型的表示。\n概率论与数理统计：评估、选择模型，数据处理，决策优化等。\n微积分：涉及到梯度下降，反向传播与正向传播，处理损失函数，进行正则化，卷积操作，一些其他的优化算法（如牛顿法、拟牛顿法等）等。\n信息论：评估特征信息量，量化模型复杂度，数据压缩等。其中的交叉熵损失函数广泛运用于分类、目标检测和NLP（自然语言处理）。\n与均方误差（MSE）相比，交叉熵损失函数在处理分类问题时通常更具优势，因为它直接衡量模型对于类别分布的拟合程度，而MSE则通过最小化预测值与真实值之间的平方差来评估模型性能，这在分类问题中可能不够直观\n综上所述，数学在AI中的运用极度广泛。它不仅为AI算法提供了理论支持，而且在模型设计、训练和优化过程中发挥着关键作用。\n2. 线性代数的核心概念 2.1 矩阵的定义与应用 矩阵是线性代数中的一个基本概念，由数排成的矩形阵列，常用于线性方程组的系数表示、线性变换等。矩阵可以在某种程度上视为一个用于存放方程系数的二维系统，并且可以通过保持奇异性的行操作化简成行阶梯形。矩阵的广泛应用是线性代数中的核心，以下是矩阵的一些关键应用：\n线性方程组：矩阵可以用来表示线性方程组，通过矩阵运算可以求解方程组的解（例如可以通过高斯消元法求解），这对于理解和设计AI算法中的优化问题至关重要。\n数据表示：在机器学习中，数据通常以矩阵的形式表示，其中每一行代表一个样本，每一列代表一个特征。这种表示方法便于算法处理和分析。\n变换：矩阵可以表示线性变换，如图像处理中的旋转、缩放等操作，这些都是计算机视觉中的基础操作。\n神经网络：在深度学习中，神经网络的权重和输入数据都以矩阵的形式存在，矩阵乘法是前向传播和反向传播中的基本操作。\n2.2 单位矩阵与逆矩阵 单位矩阵和逆矩阵是矩阵理论中的重要概念，它们在解决线性方程组和线性变换中扮演着关键角色。\n单位矩阵：单位矩阵是一个方阵，其主对角线上的元素都是1，其余元素都是0。单位矩阵与任何向量执行点积，其结果等于该向量。对单位矩阵做线性变换得到的基向量不变，这是因为单位矩阵代表了线性变换中的“无操作”。\n逆矩阵：逆矩阵与原矩阵的乘积等于单位矩阵。逆矩阵可以通过解方程计算得到。非奇异的矩阵总是有逆的（可逆矩阵），而奇异的矩阵总是无逆的。奇异的方程的行列式必然为0，这就像是数字0没有逆元一样。互为逆矩阵的矩阵的行列式互为倒数。刚好1/0是未定义的，奇异的矩阵无逆矩阵。\n$$ det({A}^{-1})=\\frac {1} {det(A)} $$ 2.3 向量的基本性质 向量是线性代数中的另一个核心概念，核心要素包括方向和大小，可以视为指向某个坐标的箭头。\n范数：向量的范数是衡量向量“长度”的一种方式。默认的L2范数是向量内所有数平方和的平方根，而曼哈顿距离（L1范数）是向量内所有数的绝对值之和。这些范数在不同的应用场景中有着不同的用途，如在优化问题中，选择合适的范数可以帮助我们得到不同的优化结果。\n正交性：当两个向量是正交的时，这两个向量的点积为0。这个性质在机器学习中的特征选择和降维中非常重要，因为它可以帮助我们识别和消除特征之间的相关性。\n投影：两个向量成角度的向量的点积，等同于其中一个向量对另一个向量做投影得到的向量与另一个向量的点积。这说明可以用投影的正负确定点积的正负，一定程度上可以理解为，点积值为正的两个向量夹角必然小于90度。\n3. 线性代数的深入理解 3.1 奇异性与非奇异性 在线性代数中，奇异性与非奇异性是描述矩阵性质的两个重要概念。它们对于理解矩阵的可逆性、解的存在性以及线性方程组的解空间至关重要。\n奇异性定义：简单而言，一个不具备冗余和矛盾信息的句子系统或方程组是非奇异的。非奇异方程组在一般情况下通常可解，而奇异方程在一般情况下通常不可解。这可以通过行列式法或秩判定法来判断。行列式法涉及将矩阵按照两个方向划分为数条对角线，若各自乘积之和相等则是奇异的。秩判定法则是看方阵的秩是否小于其阶数，若是，则矩阵是奇异的。\n数据科学中的应用：在数据科学中，奇异性与非奇异性的概念对于理解数据集的线性独立性非常重要。一个非奇异的数据矩阵意味着数据集中没有冗余的特征，这有助于避免在机器学习模型中出现过拟合现象。\n数值计算中的影响：在数值计算中，奇异矩阵可能导致算法的不稳定和数值误差的放大。例如，在求解线性方程组或者进行矩阵求逆时，奇异矩阵可能会导致算法失败或者结果不准确。\n3.2 秩的概念及其重要性 秩是矩阵的一个基本属性，它描述了矩阵中线性无关的行或列的最大数量，反映了矩阵所包含的“有效”信息的多少。\n秩的定义：矩阵的秩是矩阵中线性无关的行（或列）向量的最大个数。它决定了线性方程组中独立方程的个数，进而影响方程组解的情况。秩的概念在信息论中也非常重要，因为它可以衡量矩阵信息量。\n秩与数据压缩：在数据压缩和降维领域，秩的概念被用来识别最重要的特征。通过降低数据矩阵的秩，我们可以去除不重要的噪声和冗余信息，从而实现数据的有效压缩。\n秩与机器学习：在机器学习中，秩的概念可以帮助我们理解模型的复杂度。例如，在主成分分析（PCA）中，我们通过选择前几个主成分来降低数据的维度，这些主成分的数目通常与数据矩阵的秩有关。\n秩与线性方程组：在线性方程组中，系数矩阵的秩决定了方程组解的性质。如果系数矩阵的秩等于增广矩阵的秩且小于变量的数目，则方程组有无穷多解；如果秩等于变量的数目，则方程组有唯一解；如果秩小于变量的数目，则方程组无解。\n4. 行阶梯形式与高斯消元法 4.1 行阶梯形式的特点 行阶梯形式是线性代数中对矩阵进行化简的一种重要形式，它通过保持奇异性的行操作对矩阵进行化简。以下是行阶梯形式的一些关键特点：\n主元位置：每一行最左边的非0数被称为主元，主元数等于秩的值。每一行的主元必然位于上一行的右方，这保证了矩阵的上三角结构。\n全零行：矩阵的全零行只能出现在矩阵的下部，且如果出现了全零行，该矩阵是奇异的。这一点是判断矩阵是否奇异的重要依据。\n化简过程：通过将每个主元所在列的其他数字化为0得到的矩阵被称为简化行梯形式。这个过程有助于进一步简化矩阵，使其更易于处理。\n非奇异性判定：当且仅当主元的数等于阶数的矩阵是非奇异的。这意味着，如果一个矩阵在其行阶梯形式中每一行和每一列都有一个主元，则该矩阵是非奇异的，反之则为奇异。\n对角线特性：在行阶梯形式中，主对角线以下的所有元素都是0，这使得矩阵的结构更加清晰，便于进行后续的计算和分析。\n4.2 高斯消元法的应用 高斯消元法是一种用于求解线性方程组的算法，它通过行操作将增广矩阵转换为行阶梯形式或简化行阶梯形式，从而得出方程组的解。以下是高斯消元法的一些关键应用：\n方程求解：高斯消元法可以将线性方程组的系数矩阵和常数项矩阵合并为一个增广矩阵，然后通过行操作将其转换为行阶梯形式，从而求解方程组。\n算法效率：高斯消元法在数值计算中非常高效，尤其是对于大规模的线性方程组。它的效率在于能够逐步消去变量，减少计算量。\n数值稳定性：高斯消元法在执行过程中可以通过部分选主元等策略来提高数值稳定性，减少计算过程中的舍入误差。\n矩阵求逆：高斯消元法也可以用来求解矩阵的逆。通过将单位矩阵与原矩阵增广，然后执行高斯消元，可以得到原矩阵的逆矩阵。\n线性代数的基础：高斯消元法是理解线性代数中许多其他概念和算法的基础，如矩阵分解、LU分解等。\n在AI中的应用：在AI领域，高斯消元法可以用于求解优化问题中的线性方程组，如在支持向量机（SVM）的训练过程中求解拉格朗日乘子。\n通过对行阶梯形式和高斯消元法的深入理解，我们可以更好地掌握线性代数在解决实际问题中的应用，尤其是在AI领域的算法实现和数据分析中。\n5. 线性代数中的运算 5.1 向量运算 向量运算是线性代数中的基础，它们在AI领域的数据处理和特征工程中扮演着重要角色。以下是向量运算的一些关键点：\n向量-向量加法：向量加法可以通过平行四边形法则来理解，其结果向量的坐标是两个向量对应坐标的和。在实际计算中，向量加法是按元素相加的。\n向量-向量减法：向量减法可以视为求两个向量差的运算，结果向量的坐标是两个向量对应坐标的差。在实际计算中，向量减法是按元素相减的。\n向量-标量乘法：向量与标量的乘法是将向量的每个元素乘以该标量，结果向量的每个元素都是原向量对应元素与标量的乘积。\n向量-向量乘法（点积）：两个向量的点积是对应元素乘积的和。点积的结果是一个标量，它具有几何意义，如计算两个向量之间的夹角和相似度。在AI中，点积常用于特征归一化和相似性度量。\n转置：向量的转置是将行向量转换为列向量，或将列向量转换为行向量。在矩阵运算中，转置操作常用于改变数据的布局以适应特定的计算需求。\n5.2 矩阵运算 矩阵运算是线性代数中的核心，它们在AI算法的实现中至关重要。以下是矩阵运算的一些关键点：\n矩阵-向量乘法：矩阵与向量的乘法是将矩阵的每一行与向量进行点积操作，结果是一个向量。在神经网络中，这种运算用于实现前向传播和反向传播。\n转置：矩阵的转置是将矩阵的行和列互换，即原矩阵的第i行第j列元素变成转置矩阵的第j行第i列元素。转置操作在AI中用于调整数据维度，如在图像处理和特征变换中。\n矩阵-矩阵乘法：矩阵乘法是通过第一个矩阵的每一行与第二个矩阵的每一列的点积来计算的。矩阵乘法在AI中用于实现复杂的变换，如卷积神经网络中的卷积操作。\n矩阵-矩阵乘法有一个特性：两个矩阵相乘再计算行列式的值会等于这两个矩阵的行列式的积。\n$$ det(AB)=det(A)⋅det(B) $$ 而因为奇异矩阵的行列式为0，因此任何矩阵与奇异矩阵相乘都会等于0。这一点也可以说明为什么奇异的矩阵没有逆元，因为单位矩阵是非奇异的。\n线性变换：矩阵可以表示线性变换，这种变换将平面上的点映射到另一个点。在AI中，线性变换用于特征提取和数据降维，如主成分分析（PCA）。\n通过对线性代数中的向量和矩阵运算的深入理解，我们可以更好地掌握这些运算在AI领域的应用，从而在算法设计和数据分析中做出更合理的决策。\n6. 线性变换及其应用 6.1 线性变换的定义 线性变换是线性代数中的一个重要概念，它描述了一种特殊的函数，这种函数将向量空间中的元素映射到同一空间或另一个向量空间中的元素，同时保持向量加法和标量乘法的操作不变。具体来说，如果有一个函数 \\( T \\) 从向量空间 \\( V \\) 映射到向量空间 \\( W \\)，对于任意向量 \\( \\mathbf{u}, \\mathbf{v} \\in V \\) 和任意标量 \\( c \\)，满足以下两个条件，则 \\( T \\) 是一个线性变换：\n$$ T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v}) $$ $$ T(c\\mathbf{u}) = cT(\\mathbf{u}) $$ 线性变换在AI领域中的应用非常广泛，包括图像处理、语音识别、自然语言处理等。例如，在图像处理中，线性变换可以用于图像的旋转、缩放和剪切等操作；在自然语言处理中，线性变换可以用于词向量的转换和文本的特征提取。\n6.2 线性变换与基向量 基向量是定义向量空间的一个关键概念，它们是一组线性无关的向量，可以用来表示空间中的任何向量。在线性变换中，基向量扮演着至关重要的角色，因为线性变换可以看作是基向量在变换下的映射。\n基向量的变换：对于一个给定的线性变换 \\( T \\)，如果我们知道基向量在 \\( T \\) 下的像，那么我们就可以确定 \\( T \\) 对空间中任何向量的作用。这是因为空间中的任何向量都可以表示为基向量的线性组合，而 \\( T \\) 的线性保证了它对线性组合的作用可以通过对基向量的作用来确定。\n$$ \\{\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_n\\} $$$$ \\{\\mathbf{w}_1, \\mathbf{w}_2, ..., \\mathbf{w}_m\\} $$$$ m \\times n $$$$ \\mathbf{v}_i $$ 在 \\( T \\) 下的像，以 \\( W \\) 的基向量表示。\n维度与秩：线性变换的秩，即变换后图像的维度，等于变换矩阵的秩。如果变换是非奇异的，那么秩等于基向量的数量，这意味着基向量在变换后仍然覆盖整个空间。如果变换是奇异的，那么秩小于基向量的数量，这意味着基向量在变换后覆盖的空间降维了。\n行列式与体积变化：对于非奇异的线性变换，行列式的绝对值表示变换后基向量形成的平行六面体的体积与变换前基向量形成的单位立方体体积的比率。如果行列式的值为零，则变换是奇异的，基向量在变换后形成的体积为零，即所有的基向量映射到了一个低维空间。\n面积特征：线性变换对于单位基向量的变换后的图像的面积等于行列式的值的绝对值。\n当行列式为负时，线条会颠倒。某种程度上可以把这视为负面积值。\n$$ S = |\\det(A)| $$ 6.3 离散动力系统 离散动力系统描述了系统状态在离散时间点上的变化。离散动力系统的状态更新是在一系列特定的时间间隔内进行的，这些时间间隔通常称为时间步长。其函数可表示为：\n$$ x_{t+1} = f(x_t) $$ 特征值和特征向量在离散动力系统和连续动力系统中都有应用，它们是数值代数的核心内容。对于一个马尔可夫矩阵。可以通过当前的状态向量与概率矩阵点积得到目标概率。当反复执行过程直到趋向于稳定，就表示得到的是特征向量且特征值为1。\n一个所有列向量的元素和的值为1且所有元素非负的方阵叫做马尔可夫矩阵。\n通过对线性变换和基向量的深入理解，我们可以更好地把握线性代数在AI领域的应用，从而在算法设计和数据分析中做出更合理的决策。线性变换提供了一种强大的工具，用于分析和处理高维数据，而基向量则为我们提供了一种直观的方式来理解和操作这些变换。\n7.张成和基 7.1 张成的概念与基的概念 ​\t张成不仅帮助我们理解向量之间的线性关系，还为定义和研究线性空间的结构提供了基础。\n张成的概念：张成是一定的向量能够通过一定数量的重复叠加所能达到的所有位置。具体来说，给定一个向量集合 S={v1,v2,…,vn}，这个集合的张成（span）就是所有可以表示为这些向量的线性组合的向量的集合。\n基的概念：基是一个最小（向量数量）的张成集。只有线性独立的向量才能构成基。基的向量数等同于该空间数的维度。\n7.2 基的特点 线性无关性：只有线性独立的向量才能构成基。线性独立指你无法通过同组的其他向量构成该向量。 关于线性独立与奇异性的联系：考虑一个 n*×n 的方阵 A，其列向量为 {v1,v2,…,vn}。 如果 A 的列向量是线性独立的，那么 A 是非奇异的，即 A 有逆矩阵。 如果 A 的列向量是线性相关的，那么 A 是奇异的，即 A 没有逆矩阵。 张成性：基中的向量可以线性组合生成整个向量空间中的任何向量。 最小性：基是包含在向量空间中的最小向量集合，它既线性无关又张成整个空间。 唯一性：对于给定的向量空间，基不是唯一的，但任何两个基都包含相同数量的向量，这个数量就是空间的维度。 7.3 特征基 通过特征向量构成的基被称为特征基。对于一个用于线性变换的矩阵，如果存在某一个（组）向量，通过该矩阵线性变换后只是缩放或反转，而非拉伸和扭曲，则称该基为这个矩阵的特征向量。可以用公式这样表示，其中A是一个矩阵，v是一个向量，λ是一个标量：\n$$ Av = \\lambda v $$ 在以上的定义公式中，λ 被称为特征向量 v的特征值。\n可以通过计算标量-向量乘法来代替矩阵-向量乘法，这有效的降低了计算量。\n特征向量具体的计算过程及原理： 因为特征向量在矩阵上只是缩放，因此变换后与变换前是线性相关的。设存在一个特殊的矩阵，其满足将单位向量放大m倍：\n$$ m = \\begin{pmatrix} m \u0026 0 \u0026 0 \\\\ 0 \u0026 m \u0026 0 \\\\ 0 \u0026 0 \u0026 m \\end{pmatrix} $$ 然后，我们假设这个m在某一条轴上与我们的目标矩阵缩放尺度相同。因为其处处相等，我们可以知道，其差是一个奇异的矩阵。那么便有：\n$$ det(A−λI)=0 $$ 这个方程被称为特征方程。解特征方程 ，得到特征值 λ1,λ2,…,λn。\n对于每个特征值 λi，我们需要找到对应的特征向量 vi。特征向量是一个非零的向量，且满足：\n$$ (A−λi*I)vi=0 $$ 对于每一个特征值vi，我们执行这个解方程操作。这个方程组可能有多个解，但任何非零解都可以作为特征向量。\n将特征向量归一化，方便使用。当解出来的特征值存在相同项时，不一定有特征基。\n8.PCA（主成分分析） ​\tPCA（主成分分析，Principal Component Analysis）是一种统计方法，它通过正交变换将一组可能相关的变量转换为一组线性无关的变量集，称为主成分。PCA 通常用于降维，数据压缩，特征提取，以及在探索性数据分析中寻找数据中的模式。\n8.1 PCA的作用 降维：减少数据的维度，降低计算复杂性，同时保留最重要的信息。\n去相关：新的特征（主成分）是线性无关的，这有助于消除原始数据中的多重共线性问题。\n数据压缩：通过保留最重要的主成分，可以有效地压缩数据。\n可视化：在高维数据集中，PCA 可以帮助将数据投影到二维或三维空间，以便于可视化。\n噪声过滤：PCA 可以通过去除数据中的噪声来提高模型的性能。\n数据预处理：在许多机器学习算法中，PCA 可以作为数据预处理步骤，以提高算法的效率和准确性。\n8.2 PCA的使用 PCA的使用过程可以分为 步。假设我们拥有数据集X，则需按如下步骤进行：\n1.中心化：将数据集中的每个特征减去其均值，使得新的数据集具有零均值。从视觉上看，这就像是将点移到了坐标轴中间。\n假设我们有一个数据集X，其中包含n个样本和m个特征。中心化的过程可以表示为：\n$$ \\mu = \\begin{pmatrix} \\frac{1}{n} \\sum_{i=1}^{n} X_{i1}\\frac{1}{n} \\sum_{i=1}^{n} X_{i2} \\\\ \\vdots \\\\ \\frac{1}{n} \\sum_{i=1}^{n} X_{im} \\end{pmatrix} $$ $$ X_{\\text{centered}} = X - \\mu $$ 2.协方差矩阵:计算中心化数据的协方差矩阵，以了解特征之间的关系。\n协方差：协方差可以方便的度量数据与数据之间对于彼此的变化趋势。公式表示如下。其中 μ 是X，Y的均值。E是期望值。\n$$ Cov(X i ​ ,X j ​ )=E[(X i ​ −μ i ​ )(X j ​ −μ j ​ )] $$ 方差：方差可以方便的衡量数据在轴上的密集程度。方差定义为定义为该随机变量与其均值（期望值）之差的平方的期望值。公式表示如下：\n$$ Var(X)=E[(X−μ) 2 ] $$ 通过协方差和方差来定义协方差矩阵，定义如下：\n$$ \\text{Cov}(x, y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\mu_x)(y_i - \\mu_y) $$ $$ C = \\text{Cov}(X) = \\begin{bmatrix} \\text{Var}(X_1) \u0026 \\text{Cov}(X_1, X_2) \u0026 \\cdots \u0026 \\text{Cov}(X_1, X_n) \\\\ \\text{Cov}(X_2, X_1) \u0026 \\text{Var}(X_2) \u0026 \\cdots \u0026 \\text{Cov}(X_2, X_n) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\text{Cov}(X_n, X_1) \u0026 \\text{Cov}(X_n, X_2) \u0026 \\cdots \u0026 \\text{Var}(X_n) \\end{bmatrix} $$ 3.特征值分解：\n在已经计算出协方差矩阵的前提下，找出协方差矩阵的特征值和特征向量（被称为主成分）。因为协方差矩阵的转置不变的特点，所以特征向量必然正交。 特征值和特征向量计算的过程在前面有涉及。 4.选择主成分：根据特征值的大小选择最重要的特征向量，特征值越大，对应的特征向量越重要。这些特征向量构成了新的特征空间。\n根据我们要的目标空间的维度，选择对应数量的特征向量。优先选择特征值大的特征向量作为投影对象。 5.转换数据：将数据投影到对应的空间上，完成PCA。\n投影的过程如下：\n乘以目标空间的张成向量可以投影到目标空间上，而除以目标空间张量的范数可以避免发生延展。\n$$ A_P = A \\frac{v}{\\|v\\|_2} $$ 8.3 PCA的局限性 线性假设：PCA 假设数据的主成分是线性的，对于非线性结构可能不适用。 对异常值敏感：PCA 对异常值非常敏感，异常值可能会对主成分产生较大影响。 9. 总结 线性代数作为数学的一个重要分支，在人工智能（AI）领域的应用至关重要。从基础的矩阵运算到复杂的线性变换，线性代数的概念和工具为AI算法的开发和优化提供了坚实的理论基础和计算框架。\n7.1 线性代数的核心作用 线性代数的核心作用体现在以下几个方面：\n理论基础：线性代数为AI算法提供了理论支撑，使得算法的实现成为可能。例如，神经网络中的权重更新和反向传播算法依赖于矩阵和向量的运算。\n数据处理：在AI中，数据通常以矩阵的形式表示，线性代数提供了处理和分析这些数据的有效工具，如特征提取和降维。\n模型优化：线性代数在模型优化中扮演着关键角色，尤其是在优化算法中，如梯度下降法，需要用到微积分和线性代数的知识。\n算法创新：许多新的AI算法，如卷积神经网络（CNN）和循环神经网络（RNN），都是基于线性代数的理论创新。\n7.2 线性代数的实际应用 线性代数的实际应用包括但不限于：\n图像处理：在计算机视觉中，线性变换用于图像的旋转、缩放和剪切等操作。\n自然语言处理：在线性代数的帮助下，可以实现词向量的转换和文本的特征提取。\n优化问题：在线性代数的支持下，可以求解优化问题中的线性方程组，如在支持向量机（SVM）的训练过程中。\n特征工程：线性代数提供了特征归一化和相似性度量的工具，这对于特征选择和降维非常重要。\n通过对线性代数的深入理解和应用，AI研究者和实践者能够更好地设计和优化算法，处理和分析数据，从而推动AI技术的发展。线性代数不仅是AI学习的基础，也是实现AI应用的关键。\n[NOTE]\n本篇存在一定AI辅助\n","date":"2025-01-01T00:00:00Z","image":"https://SJTdreams.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/title_hu5207916475501752806.png","permalink":"https://SJTdreams.github.io/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"机器学习线性代数学习笔记"},{"content":"更新日志 2024.12.29 博客创建完毕 2025.1.1 博客同步和发布完毕，基本功能齐全 2025.1.2 加入标签云，加入画廊，加入音乐播放器 ","date":"2024-12-29T12:41:22+08:00","image":"https://SJTdreams.github.io/p/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/title_hu16500590770764061160.jpeg","permalink":"https://SJTdreams.github.io/p/%E6%9B%B4%E6%96%B0%E6%97%A5%E5%BF%97/","title":"更新日志"}]